# TP Kafka â€“ Spring Cloud Stream & Kafka Streams

Ce projet illustre lâ€™utilisation de **Kafka** avec **Spring Boot / Spring Cloud Stream** pour construire une chaÃ®ne complÃ¨te de traitement temps rÃ©el :

- Installation et test de base de Kafka
- Services Producer / Consumer / Supplier
- Traitement temps rÃ©el avec Kafka Streams
- Application Web pour visualiser les rÃ©sultats en direct

---

## 1. TÃ©lÃ©chargement & Installation de Kafka

TÃ©lÃ©chargement dâ€™Apache Kafka, dÃ©compression et prÃ©paration de lâ€™environnement.

---

## 2. DÃ©marrage de Zookeeper

Lancement du service **Zookeeper** nÃ©cessaire Ã  Kafka :

- Soit via script `zookeeper-server-start.sh` / `.bat`
- Soit via `docker-compose` (service `zookeeper`)

---

## 3. DÃ©marrage de Kafka Server

DÃ©marrage du **Kafka Broker** :

- Script `kafka-server-start.sh` / `.bat`
- Ou service `broker` dans `docker-compose.yml`


---

## 4. Test avec kafka-console-producer & kafka-console-consumer

CrÃ©ation dâ€™un topic de test et envoi/rÃ©ception de messages via la ligne de commande :

- `kafka-console-producer` pour publier des messages
- `kafka-console-consumer` pour les consommer

**Capture dâ€™Ã©cran :**
> ðŸ“· ![Console Producer](screenshots/producer.png)
> ðŸ“· ![Console Consumer](screenshots/consumer.png)

---

## 5. Service Producer Kafka (REST Controller)

Mise en place dâ€™un **service REST** qui joue le rÃ´le de **Producer** :

- Endpoint `/publish` (ex. `GET /publish?name=xxx&topic=T2`)
- Utilisation de `StreamBridge` pour envoyer un `PageEvent` vers un topic Kafka

**Capture dâ€™Ã©cran :**
> ðŸ“· ![REST Producer](screenshots/rest-producer.png)

---

## 6. Service Consumer Kafka

CrÃ©ation dâ€™un **Consumer** avec Spring Cloud Stream :

- DÃ©claration dâ€™un `@Bean Consumer<PageEvent>`
- Lecture des messages Ã  partir du topic (ex. `T2`)
- Affichage du contenu dans la console

**Capture dâ€™Ã©cran :**
> ðŸ“· ![Kafka Consumer](screenshots/kafka-consumer.png)

---

## 7. Service Supplier Kafka

Mise en place dâ€™un **Supplier** qui gÃ©nÃ¨re pÃ©riodiquement des Ã©vÃ©nements :

- `@Bean Supplier<PageEvent>`
- Envoi automatique dâ€™Ã©vÃ©nements vers un topic Kafka
- Utilisation possible dâ€™un intervalle configurÃ© (fixedDelay, etc.)

**Capture dâ€™Ã©cran :**
> ðŸ“· ![Kafka Supplier](screenshots/supplier.png)

---

## 8. Service de Data Analytics â€“ Kafka Streams

ImplÃ©mentation dâ€™un traitement temps rÃ©el avec **Kafka Streams** :

- Lecture des Ã©vÃ©nements depuis un topic source
- AgrÃ©gation / comptage par fenÃªtre de temps (`windowedBy(TimeWindows...)`)
- Ã‰criture des rÃ©sultats dans un topic de sortie (ex. `T4`)

**Capture dâ€™Ã©cran :**
> ðŸ“· ![Kafka Streams Processing](screenshots/agg.png)

---

## 9. Application Web â€“ Visualisation Temps RÃ©el

CrÃ©ation dâ€™une petite **application Web** qui :

- Consomme les rÃ©sultats du topic dâ€™analytics
- Affiche les statistiques en temps rÃ©el (tableau, graphiques, etc.)

**Capture dâ€™Ã©cran :**
> ðŸ“· ![Real-time Web UI](screenshots/real-time.png)
> ðŸ“· ![Real-time Web UI](screenshots/graph.png)

---

## Lancement du Projet

1. DÃ©marrer Zookeeper & Kafka (ou `docker-compose up`).
2. Lancer lâ€™application Spring Boot.
3. Tester :
    - Producer REST : `http://localhost:8085/publish?name=Test&topic=T2`
    - VÃ©rifier la consommation dans la console (Consumer / Streams).
    - Ouvrir lâ€™UI Web pour voir les rÃ©sultats temps rÃ©el.

---